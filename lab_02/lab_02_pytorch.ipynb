{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"center\" style=\"max-width: 1000px\" src=\"https://raw.githubusercontent.com/HSG-AIML-Teaching/AI2024-Lab/main/banner.png?raw=1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"right\" style=\"max-width: 200px; height: auto\" src=\"https://raw.githubusercontent.com/HSG-AIML-Teaching/AI2024-Lab/main/hsg_logo.png?raw=1\">\n",
    "\n",
    "##  Lab 02 - \"PyTorch and Tensor Processing\"\n",
    "\n",
    "Artificial Intelligence (Spring 2024), University of St. Gallen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we will learn about tensors and some basics of tensor processing using PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# !pip install torch torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.0 Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors are a type of data structure used in linear algebra, and they are especially important in various fields of artificial intelligence, including deep learning. Essentially, tensors are a generalization of scalars, vectors, and matrices to higher dimensions.\n",
    "\n",
    "Here's how you can think of different tensor dimensions:\n",
    "\n",
    "A 0-dimensional tensor is a scalar (a single number).\n",
    "A 1-dimensional tensor is a vector (a list of numbers).\n",
    "A 2-dimensional tensor is a matrix (a 2D array of numbers).\n",
    "A 3-dimensional tensor and higher are just extensions of these concepts to more dimensions.\n",
    "Tensors are key in machine learning and deep learning because they allow for the efficient storage and manipulation of data sets that are essential for training models.\n",
    "\n",
    "In PyTorch, a popular deep learning framework, tensors are used extensively. Let's see how you can create and manipulate tensors in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scalar (0D tensor): tensor(5)\n",
      "Vector (1D tensor): tensor([1, 2, 3])\n",
      "Matrix (2D tensor): tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6]])\n",
      "3D Tensor: tensor([[[1, 2],\n",
      "         [3, 4]],\n",
      "\n",
      "        [[5, 6],\n",
      "         [7, 8]]])\n",
      "Shapes of tensors: torch.Size([]) torch.Size([3]) torch.Size([3, 2]) torch.Size([2, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "# A 0-dimensional tensor (scalar)\n",
    "scalar = torch.tensor(5)\n",
    "print(\"Scalar (0D tensor):\", scalar)\n",
    "\n",
    "# A 1-dimensional tensor (vector)\n",
    "vector = torch.tensor([1, 2, 3])\n",
    "print(\"Vector (1D tensor):\", vector)\n",
    "\n",
    "# A 2-dimensional tensor (matrix)\n",
    "matrix = torch.tensor([[1, 2], \n",
    "                       [3, 4], \n",
    "                       [5, 6]])\n",
    "print(\"Matrix (2D tensor):\", matrix)\n",
    "\n",
    "# A 3-dimensional tensor\n",
    "tensor_3d = torch.tensor([\n",
    "    [[1, 2], \n",
    "     [3, 4]], \n",
    "    [[5, 6], \n",
    "     [7, 8]]\n",
    "])\n",
    "print(\"3D Tensor:\", tensor_3d)\n",
    "\n",
    "# Show the shapes of each tensor\n",
    "print(\"Shapes of tensors:\", scalar.shape, vector.shape, matrix.shape, tensor_3d.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0 A Comparison Between NumPy and PyTorch:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch and NumPy are both powerful libraries in Python, but they serve different purposes and have different strengths.\n",
    "\n",
    "Array vs. Tensor: NumPy provides support for large, multi-dimensional arrays and matrices, while PyTorch provides multi-dimensional arrays called tensors with strong support for deep learning and computational graph dynamics.\n",
    "Computational Graphs and Gradients: PyTorch tensors can be part of a computational graph, and they can keep track of the gradient - something NumPy arrays can't do.\n",
    "GPU Support: PyTorch tensors can be moved to a GPU in order to perform massively parallel, fast computations. NumPy, on the other hand, operates only on the CPU.\n",
    "API and Usage: While PyTorch and NumPy share many similar operations, their APIs differ, and PyTorch's is more aligned with deep learning workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    ".beautifulTable {\n",
    "    width: 100%;\n",
    "    border-collapse: collapse;\n",
    "    text-align: center;\n",
    "}\n",
    ".beautifulTable th, .beautifulTable td {\n",
    "    padding: 10px;\n",
    "    border: 1px solid #ddd;\n",
    "}\n",
    ".beautifulTable th {\n",
    "    background-color: #4CAF50;\n",
    "    color: white;\n",
    "}\n",
    ".beautifulTable tr:nth-child(even) {\n",
    "    background-color: white;\n",
    "    color: black;\n",
    "}\n",
    ".beautifulTable tr:hover {\n",
    "    background-color: yellow;\n",
    "    color: black;\n",
    "\n",
    "}\n",
    "</style>\n",
    "\n",
    "<table class=\"beautifulTable\">\n",
    "    <tr>\n",
    "        <th>Feature</th>\n",
    "        <th>NumPy</th>\n",
    "        <th>PyTorch</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><strong>Core Data Structure</strong></td>\n",
    "        <td>Multi-dimensional array (ndarray)</td>\n",
    "        <td>Multi-dimensional array (Tensor)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><strong>Computational Backend</strong></td>\n",
    "        <td>CPU only</td>\n",
    "        <td>CPU and GPU</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><strong>Dynamic Computational Graph</strong></td>\n",
    "        <td>Not available</td>\n",
    "        <td>Available (Dynamic computation graph for backpropagation)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><strong>Automatic Differentiation</strong></td>\n",
    "        <td>Not available</td>\n",
    "        <td>Available (through the autograd system)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><strong>Data Handling for AI</strong></td>\n",
    "        <td>Basic array operations</td>\n",
    "        <td>Comprehensive support for data loading, transformations, and batching for AI</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><strong>Parallel Computing</strong></td>\n",
    "        <td>Limited (through libraries like Dask)</td>\n",
    "        <td>Extensive (native GPU support for parallel computing)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><strong>Memory Management</strong></td>\n",
    "        <td>Manual control and efficiency</td>\n",
    "        <td>Advanced (with in-place operations, shared memory)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><strong>Interoperability</strong></td>\n",
    "        <td>Extensive (used as a base for many scientific computing in Python)</td>\n",
    "        <td>Can convert between NumPy arrays and PyTorch tensors easily</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><strong>API Consistency</strong></td>\n",
    "        <td>Stable API, widely used in scientific computing</td>\n",
    "        <td>Designed for deep learning, changes more frequently</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><strong>Speed</strong></td>\n",
    "        <td>Fast for array operations on CPU</td>\n",
    "        <td>Faster for large-scale operations, especially on GPU</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><strong>Community and Support</strong></td>\n",
    "        <td>Very large user community, extensive documentation</td>\n",
    "        <td>Growing rapidly, especially among researchers and AI practitioners</td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What features does PyTorch offer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"center\" style=\"max-width: 700px\" src=\"images/pytorch_packages.jpg\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ `autograd`: This package is used for automatic differentiation. The autograd package is essential for training neural networks using backpropagation, as it allows users to easily compute gradients of the loss function with respect to the model parameters.\n",
    "\n",
    "+ `nn`: This package provides a high-level API for building neural networks in PyTorch. It includes the most common types of layers such as convolutional layers, pooling layers, and linear layers, as well as activation functions and loss functions. The `nn` module also provides tools for building custom layers and models using PyTorch tensors.\n",
    "\n",
    "+ `optim`: This package provides various optimization algorithms for training neural networks in PyTorch. It includes popular optimization methods such as Stochastic Gradient Descent (SGD), Adam, and Adagrad. The optim module also provides tools for customizing the learning rate and weight decay, as well as implementing learning rate schedulers.\n",
    "\n",
    "+ `utils`: This package provides a variety of utility functions such as data loading and and visualization. For example, the `torch.utils.data` module contains classes and functions for loading and preprocessing data, and the `torch.utils.tensorboard` module provides support for visualizing training and validation metrics in via `TensorBoard`. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.0 Transferring tensors between compute devices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily transfer a tensor to the target device by calling `.to(DEVICE_NAME)` on the tensor directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute device - before:  cpu\n",
      "Compute device - after:  mps:0\n"
     ]
    }
   ],
   "source": [
    "matrix = torch.tensor([[1, 2], \n",
    "                       [3, 4], \n",
    "                       [5, 6]])\n",
    "\n",
    "\n",
    "print(\"Compute device - before: \", matrix.device)\n",
    "\n",
    "# Transfer the tensor to the GPU\n",
    "matrix = matrix.to(\"cuda\") # CUDA-compatible GPU\n",
    "# matrix = matrix.to(\"mps\")   # MPS-compatible GPU - Macbooks with M-series chips\n",
    "\n",
    "print(\"Compute device - after: \", matrix.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.0 Computational Graphs and Automatic Differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What are computational graphs and why do we need them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A computational graph in a directed acyclic graph (DAG) that represents the flow of information through the network. It consists of nodes that represent mathematical operations and edges that represent the flow of data between the nodes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume we have a very simple function:\n",
    "\n",
    "$$f(x) = w \\times x + b$$\n",
    "\n",
    "Here $x$ is the input and $w$ and $b$ are (learnable) parameters. We want to change $w$ and $b$ such that the output of the function gets as close as possible to a target output (ground-truth). We (randomly) initialize $w=0.2$ and $b=0.0$.\n",
    "\n",
    "Now let's calculate $f(0.4)$ in PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0800, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Example input and output\n",
    "x = torch.tensor(0.4)  # input tensor\n",
    "y = torch.tensor(1.0)  # expected output\n",
    "\n",
    "# Initialize w and b with random value (here we set them to 0.2 and 0.0)\n",
    "w = torch.tensor(0.2, requires_grad=True) # requires_grad=True -> learnable parameter\n",
    "b = torch.tensor(0.0, requires_grad=True) # requires_grad=True -> learnable parameter\n",
    "\n",
    "# Calculate f(x)\n",
    "z = w * x + b\n",
    "\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume we want $f(0.4)=1.0$, but currently  $f(0.4)=0.08$:\n",
    "\n",
    "$f(0.4) = 0.08$ $\\color{red}{\\neq}$ $\\color{green}{f(0.4) = 1.0}$\n",
    "\n",
    "To do so, we first mesaure the difference between the desired output and the actual output of the function and we call it the loss ($l$):\n",
    "\n",
    "$$l = ||f(0.4) - 1.0||_{2}^{2}$$\n",
    "\n",
    "Then, to estimate the amount of required change in $w$ and $b$ to get closer to the desired value, we need to compute the gradients of the loss w.r.t. the functions parameters:\n",
    "\n",
    "$$\\frac{\\partial l}{\\partial w}, \\frac{\\partial l}{\\partial b}$$\n",
    "\n",
    "And finally update $w$ an $b$ using gradient descent:\n",
    "$$w_{new} \\leftarrow w - \\alpha  \\frac{\\partial l}{\\partial w}$$\n",
    "$$b_{new} \\leftarrow b - \\alpha \\frac{\\partial l}{\\partial b}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9200, grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss = torch.norm(z - y, p=2)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the tensor operations above creates the following computational graphs that enables automatic differentiation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"center\" style=\"max-width: 700px\" src=\"images/comp-graph.png\">\n",
    "\n",
    "<sup> Image adapted from: <a href=\"https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html\">https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html</a> <sup>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Technically, the computational graph of the function above is created dynamically or on-the-fly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the loss is computed and the computational graphs is formed (in the background), we can compute the gradients for the learnable parmeters. But first let's check what are the gradient values for the (learnable) parameters $w$ and $b$ before computing the gradients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The easiest way to compute all gradients in a computational graphs is to call `.backward()` on the loss terms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's check the gradients again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.4000)\n",
      "tensor(-1.)\n"
     ]
    }
   ],
   "source": [
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "voila! the gradients are there. Remember that after calling `.backward()` the computational graph is removed for computational reasons. For most application you don't need to keep the computational graph, but there are ways to keep it which is outside the scope of this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Another way to compute gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute partial derrivatives w.r.t. particular parameters in the model directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(0.4)  # input tensor\n",
    "y = torch.tensor(1.0)  # expected output\n",
    "w = torch.tensor(0.2, requires_grad=True) # requires_grad=True -> learnable parameter\n",
    "b = torch.tensor(0.0, requires_grad=True) # requires_grad=True -> learnable parameter\n",
    "z = x * w + b\n",
    "loss = torch.norm(z - y, p=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.4000), tensor(-1.))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.grad(loss, [w, b])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.0 Slicing and Indexing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slicing, indexing and reshaping is done similar to NumPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First row:  tensor([1, 2])\n",
      "First column:  tensor([1, 3, 5])\n",
      "Last column:  tensor([2, 4, 6])\n",
      "\n",
      "Reshaped Tensor:\n",
      " tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6]])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.tensor([[1, 2], \n",
    "                       [3, 4], \n",
    "                       [5, 6]])\n",
    "\n",
    "# Indexing and slicing\n",
    "print(\"\\nFirst row: \", tensor[0])\n",
    "print(\"First column: \", tensor[:, 0])\n",
    "print(\"Last column: \", tensor[:, -1])\n",
    "\n",
    "# Reshaping\n",
    "reshaped_tensor = tensor.view(3, 2)\n",
    "print(\"\\nReshaped Tensor:\\n\", reshaped_tensor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.0 Tensor Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also transpose a 2D tensor and compute the sum of its elements by directly calling operators on the tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transposed Tensor:\n",
      " tensor([[1, 3, 5],\n",
      "        [2, 4, 6]])\n",
      "\n",
      "Sum of elements: tensor(21)\n",
      "\n",
      "Sine of Tensor:\n",
      " tensor([[ 0.8415,  0.9093],\n",
      "        [ 0.1411, -0.7568],\n",
      "        [-0.9589, -0.2794]])\n"
     ]
    }
   ],
   "source": [
    "# Matrix operations\n",
    "transposed_tensor = tensor.t()\n",
    "print(\"\\nTransposed Tensor:\\n\", transposed_tensor)\n",
    "\n",
    "# Reduction operations\n",
    "sum_tensor = tensor.sum()\n",
    "print(\"\\nSum of elements:\", sum_tensor)\n",
    "\n",
    "# Element-wise operations\n",
    "sin_tensor = torch.sin(tensor)\n",
    "print(\"\\nSine of Tensor:\\n\", sin_tensor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.0 Concatenating and Splitting Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For many applications we will need to concatenate two tensors or split an existing tensor into chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Concatenated Tensor:\n",
      " tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6],\n",
      "        [1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6]])\n",
      "\n",
      "Split Tensors: (tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6]]),)\n"
     ]
    }
   ],
   "source": [
    "# Concatenation\n",
    "concatenated_tensor = torch.cat([tensor, tensor], dim=0)\n",
    "print(\"\\nConcatenated Tensor:\\n\", concatenated_tensor)\n",
    "\n",
    "# Splitting\n",
    "split_tensors = torch.split(tensor, split_size_or_sections=3, dim=0)\n",
    "print(\"\\nSplit Tensors:\", split_tensors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.0 In-Place Operations and Operations Without Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually, when we perform an operation on a tensor a new tensor is created to store the output of the operation. Some operations can be done \"in-place\" that directly change the value of the original tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original tensor: tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6]])\n",
      "Tensor after in-place addition: tensor([[2, 3],\n",
      "        [4, 5],\n",
      "        [6, 7]])\n"
     ]
    }
   ],
   "source": [
    "# In-place operations\n",
    "print(\"\\nOriginal tensor:\", tensor)\n",
    "tensor.add_(1)  # In-place addition\n",
    "print(\"Tensor after in-place addition:\", tensor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, PyTorch keeps track of all operations in the background for gradient computation purposes. We can also run operations without gradient tracking, for memory efficiency reasons, using the `torch.no_grad()` context manager:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor operations without gradient tracking:\n",
      " tensor([2., 4., 6.])\n"
     ]
    }
   ],
   "source": [
    "# Using no_grad for memory efficiency\n",
    "tensor_with_grad = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "with torch.no_grad():\n",
    "    out = tensor_with_grad * 2\n",
    "    print(\"Tensor operations without gradient tracking:\\n\", out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor operations with gradient tracking:\n",
      " tensor([2., 4., 6.], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "out = tensor_with_grad * 2\n",
    "print(\"Tensor operations with gradient tracking:\\n\", out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
